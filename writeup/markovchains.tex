\documentclass{amsart}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{csquotes}
\usepackage{todonotes}
\usepackage{verbatim}
\usepackage{amssymb}

\usepackage{tikz}
\usepackage{tikz,fullpage}
\usetikzlibrary{arrows,%
                petri,%
                topaths}%
\usepackage{tkz-berge}
\usepackage[position=top]{subfig}




\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}


\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{example}[thm]{Example}
\newtheorem{theorem}{Theorem}

\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}

\newtheorem{remark}[thm]{Remark}

\newtheorem{corollary}{Corollary}[theorem]

%%%
%%% The following, if uncommented, numbers equations within sections.
%%% 

\numberwithin{equation}{section}


\title{Regular Transition Matrices}
\author{Joseph Tobin}
\date{27 November 2017}

\begin{document}

\maketitle

%not sure why E_labda subset of prime

%include abstract

%5.17 - end
%presentation the week after break
%email rough draft over beak
%lookup and use jordan canonical form without proof

\section{Introduction}
In this paper, we explore properties of regular transition matrices using the guidance of Friedberg, Insel, and Spence \cite{friedberg2003linear}.
We will start off with basic definitions and then dive into theories, all of which are shown with sufficient proofs.
Then, we will will deduce the relevant theorems and develop a whole swath of properties for square regular transition matrices.
Finally, we will explore applications of these theorems.


We rely heavily on results shown in previous parts of section 5.3 in addition to results shown section in 5.1 and 5.2 of \cite{friedberg2003linear}.
Additionally, previous knowledge of Jordan Canonical Form is assumed.

\section{Basic Definitions and Notes}

\begin{definition}[Matrix Limit]
Let $L$, $A_1$, $A_2$, $\ldots$, be $n \times p$ matrices having complex entries.
The sequence $A_1, A_2, \ldots$ is said to \textbf{converge} to the $n \times p$ matrix $L$, called the \textbf{limit} of the sequence, if $\forall 1 \leq i \leq n$ and $\forall 1 \leq j \leq p$

$$lim_{m \to \infty}(A_m)_{ij} = L_{ij}.$$

In this case, we write $$ lim_{m \to \infty}(A_m) = L.$$


\end{definition}


\begin{definition}[Transition Matrix, Probability Vector]
We call an $n \times n$ $M$ a \textbf{transition matrix} if it contains only nonnegative entries and all of its columns sum to $1$.
We call a column vector $P$ a \textbf{probability vector} if it contains only nonnegative entries that sum to $1$.
\end{definition}

\begin{remark}[Theorem 5.15]
\begin{enumerate}
	\item $M$ is a transition matrix if and only if $M^tu = u$.
	\item $v$ is a probability vector if $u^tv = (1)$.
	\item The product of two transition matrices is a transition matrix.
	\item The product of a transition matrix and a probability vector is a probability vector.
\end{enumerate}

\end{remark}




\begin{definition}[Regular]
A transition matrix $M$ is called regular if there exists an $s \in \mathbb{N}_{>0}$ such that $M^s$ has only positive entries.

\end{definition}

\begin{definition}[Row Sum, Column Sum]
Let $A \in M_{n \times n }(C)$.

$\forall 1 \leq i, j \leq n$, define $\rho_i(A)$ to be the sum of the absolute values of the entries of row $i$ of $A$ or 

$$ \rho_i(A) = \sum_{j=1}^n|A_{ij}| $$

and define $\nu_j(A)$ to be the sum of the absolute values of the entries of column $j$ of $A$ or 

$$ \nu_j(A) = \sum_{j=1}^n|A_{ij}| $$

Then the \textbf{row sum} of A or $\rho(A)$ and the \textbf{column sum} or $\nu(A)$ are defined as:

$$ \rho(A) = max\{\rho_i(A) \forall 1 \leq i \leq n \} $$
$$ \nu(A) = max\{\nu_j(A) \forall 1 \leq j \leq n \} $$


\end{definition}




\section{Theorems}

\begin{lemma}[Section 6 Exercise 15(b)]
Suppose $V$ is an inner product space.
Show $|| x + y|| = || x|| + ||y||$ if and only if $x$ is a non-negative multiple of $y$.
\end{lemma}

\begin{proof}
$\rightarrow$ Suppose $x = cy$
Then $$|| x  + y|| = ||cx + y|| = || (c +1) y|| = |c + 1| ||y|| = (c + 1) ||y|| =  |c| ||y|| + ||y|| = ||cy || + ||y|| = ||x|| + ||y||$$


$\leftarrow$ We know by the proof of the triangle inequality (Theorem 6.2 (d) \cite{friedberg2003linear}) that $||x+y||^2 = (||x|| + ||y||)^2$ only when 
$$Re(\langle x, y \rangle) = |\langle x, y \rangle| = ||x|| * ||y||$$
where $Re(x)$ is the real part of a complex number $x$.

But this only holds if $x = cy$ for some nonnegative $c$.


\end{proof}

\begin{remark}
Then $\forall n \in \mathbb{N}$, consider $||\sum_{i=1}^n x_i||$.
Then by the previous lemma $x_n = c_n* \sum_{i = 1}^{n-1} x_i$ if and only if

$$||\sum_{i = 1}^n x_i|| = ||\sum_{i = 1}^{n-1} x_i|| + ||x_n||$$.

Then we can repeat to get $||x_{n-1}|| = c_{n-1}  * \sum_{i = 1}^{n-2} x_i$ if and only if $$ ||\sum_{i = 1}^{n-1} x_i|| =  ||\sum_{i = 1}^{n-2} x_i|| + ||x_{n-1}||  $$

And so on until we get $x_2 =\ c_2x_1 , x_3 =\ c_3x_1 \ldots x_n =\ c_nx_1\ $ if and only if  $$||\sum_{i = 1}^n x_i|| = \sum_{i = 1}^{n} ||x_i||$$ 

for non-negative scalars $c_1, c_2, \ldots c_n$.

\end{remark}


\begin{theorem}[Theorem 5.18 pg. 298]
Let $A \in M_{n \lambda n} (C)$ be a matrix in which each entry is positive and let $\lambda$ be an eigenvalue of $A$ such that $| \lambda | = \rho(A)$.
Then $\lambda = \rho(A)$ and $\{ u \}$ is a basis for $E_{\lambda}$, where $u \in C^n$ is the column vector in which each coordinate equals $1$.

\end{theorem}

\begin{proof}
First, note that because $A$ has all postitive values and $u$ has all positive values, then $Au$ has all postive values. Therefore, because $\lambda u = Au$, $\lambda u$ has all positive values.
Since $u$ has each coordinate as $1$, we can conclude $\lambda > 0$ and thus $\lambda = |\lambda| = \rho(A)$

Now we want to show that $\{ u \}$ is a basis for $E_{\lambda}$.
To show this, we are going to fact that $|\lambda| = p(A)$ to derive several equalities giving us information about $A$.

First, let $v$ be an eigenvector of $A$ corresponding to $\lambda$ with coordinates $v_1, v_2, \ldots, v_n$.  
Then let $v_k = max(\ |v_1|, |v_2|, \ldots |v_n|\ )$ and $b = |v_k|$.

Then $$ |\lambda|\ b =\ |\lambda|\ |v_k| = |\lambda v_k| $$

But if $\lambda$ is an eigenvalue of $A$, then $Av = \lambda v$ and thus $\forall\ 1 \leq i \leq n$, $\lambda v_i = \sum_{j = 1}^n A_{ij}v_j$.
Thus

$$ |\lambda v_k| = | \sum_{j = 1}^n A_{kj}v_j | $$

By the triangle inequality and then absolute value multiplication rules,

$$ | \sum_{j = 1}^n A_{kj}v_j | \leq \sum_{j=1}^n |A_{kj}v_j| = \sum_{j=1}^n |A_{kj}| |v_j| $$

Since we know $b = |v_k| \geq v_j\ \forall\ 1 \leq j \leq n$ and similarly $\rho(A) \geq  \rho_i(A)\ \forall\ 1 \leq i \leq n $, we know 

$$ \sum_{j=1}^n |A_{kj}| |v_j|  \leq \sum_{j=1}^n |A_{kj}| b = b \sum_{j=1}^n |A_{kj}| =  b\rho_k(A) = \rho_k(A)b \leq \rho(A)b $$


But since we know $|\lambda| = \rho(A)$, we know the three inequalities used above are actually equalities.

\begin{enumerate}

	\item $$| \sum_{j = 1}^n A_{kj}v_j | = \sum_{j=1}^n |A_{kj}v_j|$$

	\item $$\sum_{j=1}^n |A_{kj}| |v_j|  = \sum_{j=1}^n |A_{kj}| b$$

	\item $$\rho_k(A)b = \rho(A)b$$ \newline

\end{enumerate}



But now we can use this to show that $\{ u \}$ is a basis for $E_{\lambda}$

By the first lemma, we know $(1)$ above holds if and only if $A_{kj}v_j$ are nonnegative multiples of some nonzero complex number $z$.
Without loss of generality, assume $|z| = 1$.
Then $\exists c_1, c_2, \ldots c_n$ such that $A_{kj}v_j = c_j z$.

By $(2)$ above, we know $\forall  1 \leq j \leq n\ , |v_j| = b$ and therefore

$$b = |v_j| = |\frac{c_jz}{A_{kj}}| = \frac{c_j}{A_{kj}} \ \forall 1 \leq j \leq n$$

Since $A_{kj}v_j = c_j z$, this gives us $v_j = bz\ \forall j$ and thus 


\begin{equation}
    v = \begin{bmatrix}
           v_{1} \\
           v_{2} \\
           \vdots \\
           v_{n}
         \end{bmatrix}
     	= \begin{bmatrix}
           bz \\
           bz \\
           \vdots \\
           bz
         \end{bmatrix}
        = bzu 
\end{equation}

Thus any eigenvector $v$ of $A$ corresponding to $\lambda$ can be expressed as a scalar multiple of $u$ and thus $\{ u \}$ is a basis for $E_{\lambda}$.


\end{proof}

\begin{corollary}[Corollary 1 pg. 299 \cite{friedberg2003linear} ]

	Let $A \in M_{n \times n}(C)$ be a matrix in which each entry is positiive and let $\lambda$ be an eigenvalue of $A$ such that $|\lambda| = \nu(A)$.
	Then  $\lambda = \nu(A)$ and $E_{\lambda}$ has dimension 1. \newline

\end{corollary}

\begin{proof}

	Consider $A^t$ and let $E_{\lambda},\ E_{\lambda}\textprime$ be the eigenspaces of $\lambda$ corresponding to $A,\ A^t$ respectively.
	We know by excercise 14 of section 5.1 in \cite{friedberg2003linear} that $A$ and $A^t$ have the same eigenvalues.
	Thus $A^t$ is a mtraix in which each entry is positive and has eigenvalue $\lambda$.
	
	Now notice that because the rows of $A$ are the columns of $A^t$ we know $\nu(A) = \rho(A^t)$ and thus $|\lambda| = \rho(A^t)$.


	Thus $A^t$ is a matrix with all positive entries and with an eigenvalue $|\lambda| = \rho(A^t)$ and by Theorem 5.18 \cite{friedberg2003linear}, the basis of $E_{\lambda}\textprime = \{ u \}$, where $u \in C^n$ is the column vector in which each coordinate contains $1$ and $\lambda = \rho(A^t) = \nu(A)$.
	
	Thus $dim(E_{\lambda}\textprime) = 1$.
	But by exercise $13$ of $5.2$ of \cite{friedberg2003linear}, we know $dim(E_{\lambda}) = dim(E_{\lambda}\textprime) = 1$.
	Thus, we have shown $\lambda = \nu(A)$ and $dim(E_{\lambda}) = 1$.

\end{proof}



\begin{corollary}[Corollary 2 pg. 299]

	Let $A \in M_{n \times n}(C)$ be a transition matrix in which each entry is positive and let $\lambda$ be an eigenvalue of $A$ such that $\lambda \neq 1$.
	Then  $|\lambda| < 1$ and the eigenspace corresonding to the eigenvalue $1$ has dimension $1$.

\end{corollary}

\begin{proof}
	We know by corollary 3 of Theorem 5.16 \cite{friedberg2003linear} that if $\lambda$ is an eigenvalue of a transition matrix, then $|\lambda| \leq 1$.
	Thus if $|\lambda| \neq 1$, then $|\lambda| < 1$.
	Suppose $|\lambda| = 1$.
	We know that the sum of each column of $A$ is 1 and thus $\nu(A) = 1$ and thus $|\lambda| = \nu(A)$.
	But this means by the previous corollary that $\lambda = \nu(A) = 1 \neq -1$.
	Thus $\lambda \neq 1$ implies $|\lambda| < 1$.

	If $A$ is a transition matrix, then by Theorem 5.17 \cite{friedberg2003linear} we know that $1$ is an eigenvalue. 
	We also know that if $A$ is a transition matrix, then given $u$ as a column vector in which each coordinate equals 1, then $A^tu = u$.

	Because $\forall i\ u_i = 1$, we know
	 $$1 = u_i = \sum_{j = 1}^nA^t_{ij}u_j = \sum_{j = 1}^nA^t_{ij}(1) =  \sum_{j = 1}^nA^t_{ij} = \rho_i(A^t) = \nu_i(A)$$

	Then $\forall i\ \nu_i(A) = 1$ and thus $\nu(A) = 1$.
	Therefore we have an all-positive-entry matrix with $1 = \lambda = \nu(A) $ and thus by the previous corollary, we know $dim(E_{\lambda}) = 1$.


\end{proof}

\begin{theorem}[Theorem 5.19 pg. 298]
Let $A$ be a regular transition matrix and let $\lambda$ be an eigenvalue of $A$.  Then 

\begin{enumerate}
	\item $|\lambda| \leq 1$
	\item If $|\lambda| = 1$, then $\lambda = 1$ and $dim(E_{\lambda}) = 1$.

\end{enumerate}

\end{theorem}

\begin{proof}

We know $(1)$ was proved in Corollary 3 of Theorem 5.16 \cite{friedberg2003linear}.

Since $A$ is regular, we know by definition $\exists s \in \mathbb{N}_{>0}$ such that $A^s$ has only positive entries.
Moreover $A^s$ and $A^{s+1}$ are transition matrices because $A$ is a transition matrix and the product of transition matrices is a transition matrix.
We now split this proof into parts:

\begin{enumerate}
	\item 
		Because $A$ is a transition matrix and the entries of $A^s$ are positive, we know the entries of $A^{s+1} = A^s(A)$ are positive.
		More specifically,
		$$A^{s+1}_{ij} = \sum_{k = 1}^nA^s_{ik}A_{kj}$$
		and thus because for any given $i, j$, $\forall\ 1 \leq k \leq n\ A_{kj} \geq 0$ and $A^s_{ik} > 0$ and $\exists\ k$ such that $A_{kj}  > 0$ (since the column sums to $1$), we can conclude $A^{s+1}_{ij} > 0$.

	\item 
		Suppose $|\lambda| = 1$, then we know by by problem $15b$ of section $5.1$ in \cite{friedberg2003linear} that if $\lambda$ is a eigenvalue of $A$, then $\lambda^s$, $\lambda^{s+1}$ are eigenvalues of $A^s$, $A^{s+1}$ respectively.  Because $|\lambda| = 1$, we know $ |\lambda^s |= |\lambda^{s+1} |= |\lambda| = 1$.

	\item
		Because each entry of $A^s$, $A^{s+1}$ is positive and both matrices are transition matrices, we know that for any eigenvalues $\lambda*$ of $A^s$, $A^{s+1}$ such that $\lambda* \neq 1$, then $|\lambda| < 1$.
		Thus because $|\lambda^s |= |\lambda^{s+1} |= 1 $, we can conclude $\lambda^s = \lambda^{s+1} = 1$ and therefore $\lambda = 1$.
	\item 

		Let $E_{\lambda}$ and $E_{\lambda}\textprime$ be the eigenspaces of $A$, $A^{s+1}$ respectively corresponding to $\lambda = 1$.
		Then $E_{\lambda} \subseteq E_{\lambda}\textprime$ and because $dim(E_{\lambda}\textprime) = 1$, we know $dim(E_{\lambda}) = 1$.
\end{enumerate}

\end{proof}


\begin{corollary}[Corollary Pg. 300]
Let $A$ be a regular transition matrix that is diagonalizble.
Then $\lim_{m \to \infty} A^m$ exists.

\end{corollary}

\begin{proof}

We know by Theorem 5.14 \cite{friedberg2003linear} that if $A \in M_{n \times n}(C)$ is diagonalizable and has every eigenvalue contained in $S$, where $S = \{ \lambda \in C: |\lambda| < 1$ or $|\lambda| = 1\}$, then $\lim_{m \to \infty} A^m$ exists.


Thus because $A$ is dianolizable by assumption, we just need to show for all eigenvalues $\lambda$ of $A$, $\lambda \in S$.
But we know by Theorem 5.19 that $\forall$ eigenvalues $\lambda$, $\lambda = 1$ or $|\lambda| < 1$ and thus $\lambda \in S$.
Thus we can conclude that $\lim_{m \to \infty} A^m$ exists.

\end{proof}



The following lemmas use Jordan Canoncial form to prove Theorem 5.20.

\begin{lemma}[Modified Exercise 21 of Section 7.2 \cite{friedberg2003linear}]
	Let $A \in M_{n \times n}(C)$ be a transition matrix.
	Since $C$ is an algebraically closed field, $A$ has a Jordan canonical form $J$ to which $A$ is similar.
	Let $P$ be an invertible matrix such that $P^{-1}AP\ =\ J$.
	Then we have the following:

	\begin{enumerate}
		\item $||A^m|| \leq 1$ for every positive integer $m$.
		\item $\exists c > 0$ such that $||J^m|| \leq c$ for every positive integer $m$.
		\item Each Jordan block of $J$ corresponding to the eigenvalue $\lambda = 1$ is a $1 \times 1$ matrix.
		\item $lim_{m \to \infty}A^m$ exists if and only if $1$ is the only eigenvalue of $A$ with absolute value $1$.
		\item 

	\end{enumerate}


\end{lemma}

\begin{proof}


	We know that $A$ has eigenvalue $1$ by Theorem 5.17 \cite{friedberg2003linear} and the characteristic polynomial of $A$ splits. 


	Thus by Theorem 7.4 \cite{friedberg2003linear}, we know $\forall\ 1 \leq i \leq k$, $dim(K_{1})$ is the multiplicity of $1$ as an eigenvalue of $A$.
	Additionally, we know $E_{\lambda} \subseteq K_{\lambda}$ by Theorem 7.1.

	But if each Jordan block of $J$ corresponding to the eigenvalue $\lambda = 1$ is a $1 \times 1$ matrix, then $dim(K_1) = 1$.
	But if $E_{\lambda} \subseteq K_1$, then $dim(E_{\lambda}) = 1$.	





\end{proof}

\begin{theorem}[Theorem 5.20]

Let $A$ be an $n \times n$ regular transition matrix. Then:

\begin{enumerate}

	\item The multiplicity of $1$ as an eigenvalue of $A$ is $1$.

	\item $\lim_{m \to \infty} A^m$ exists.

	\item $L = \lim_{m \to \infty} A^m$ is a transition matrix.

	\item $LA = AL = L$.

	\item The columns of $L$ are identical and equal to the unique probability vector $v$ that is equal to the eigenvalue $1$.

	\item For any probability vector $w$, $\lim_{m \to \infty} A^m w = v$.

\end{enumerate}

\end{theorem}


\begin{proof}

\begin{enumerate}

	\item See exercise 20 of section 7.2

	\item 	1, 5.13, 5.19

	\item 
			By theorem 5.15 \cite{friedberg2003linear}, we know $L$ is a transition matrix if $L^tu = u$, where $u$ is the column vector where every entry is equal to $1$
			Due to the equivalence, we can  know $L$ is a transition matrix is  $u^tL = u^t$.
			But 
			$$u^tL = u^t \lim_{m \to \infty} A^m = \lim_{m \to \infty} u^t A^m$$

			But $A^m$ is a transition matrix which means $\forall m u^t A^m = u^t$ and thus
			$$\lim_{m \to \infty} u^t A^m = \lim_{m \to \infty} u^t = u^t $$

			Thus $L$ is a transition matrix.


	\item 

			By Theorem 5.12 \cite{friedberg2003linear}, we know $AL = \lim_{m \to \infty} A A^m = \lim_{m \to \infty} A^{m+1} = \lim_{m \to \infty} A^mA  = LA$.

			But $\lim_{m \to \infty} A^{m+1} = \lim_{m \to \infty} A^{m} = L$.

			Thus $LA = AL = L$.


	\item 
			We know $AL = L$ by $(4)$.
			Let $L_i$ be the $ith$ column vector of $L$.
			Then because $AL = L$, we know $AL_i = L_i \forall 1 \leq i \leq n$.
			Thus $L_i$ is an eigenvector of $A$ corresponding to eigenvalue $\lambda = 1$.

			Additionally, because $L$ is a transition matrix by $(3)$, we know $L^tu = u$ and thus $\forall 1 \leq i \leq n L_i^t u = u$.
			Thus $\forall 1 \leq i \leq n L_i$ is a probability vector.

			But then using $(1)$, we know that the multiplicity of $1$ as an eigenvalue of $A$ is 1.
			Thus all the columns of $L$ have to be a scalar multiple of the same vector.
			But because every column is a probability vector, they have to be the same in order to satisify $\forall 1 \leq i \leq n L_i^t u = u$. \todo{need to prove?}

			Thus each column of $L$ is equal to the the unique probability vector $v$ corresponding to eigenvalue $1$.

	\item 
		Let $w$ be any probability vector and $y = \lim_{m \to \infty} A^m w  = Lw$.
		We want to show $y = v$.
		,
		If $y = Lw$, then by the corollary to Theorem 5.15 \cite{friedberg2003linear}, we know $y$ is a probability vector.

		Additionally,
		$$Ay = A(Lw) = (AL)w = Lw$$
		by part $(4)$.
		But $Lw = y$ and thus $Ay = y$.

		Therefore $y$ is an probability vector and eigenvector of $A$ corresponding to $\lambda = 1$
		But $v$ is the unique probability vector and eigenvector of $A$ corresponding to $\lambda = 1$ and thus $y = v$.

		Thus $\lim_{m \to \infty} A^m w = v$.


\end{enumerate}


\end{proof}

\section{Applications}

\bibliographystyle{alpha}
\bibliography{MathCitations} 


\end{document}